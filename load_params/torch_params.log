Diffuser(
  (diffusion_module): DiffusionHead(
    (pair_cond_initial_norm): LayerNorm()
    (pair_cond_initial_projection): Linear(in_features=267, out_features=128, bias=True)
    (pair_transition_blocks): ModuleList(
      (0-1): 2 x TransitionBlock(
        (adaptive_layernorm): AdaptiveLayerNorm(
          (layer_norm): LayerNorm()
          (single_cond_layer_norm): LayerNorm()
          (single_cond_scale): Linear()
          (single_cond_bias): Linear()
        )
        (ffw_transition1): Linear(in_features=128, out_features=512, bias=True)
        (adaptive_zero_init): AdaptiveZeroInit(
          (transition2): Linear()
          (adaptive_zero_cond): Linear()
        )
      )
    )
    (single_cond_initial_norm): LayerNorm()
    (single_cond_initial_projection): Linear(in_features=831, out_features=384, bias=True)
    (noise_embedding_initial_norm): LayerNorm()
    (noise_embedding_initial_projection): Linear(in_features=256, out_features=384, bias=True)
    (single_transition_blocks): ModuleList(
      (0-1): 2 x TransitionBlock(
        (adaptive_layernorm): AdaptiveLayerNorm(
          (layer_norm): LayerNorm()
          (single_cond_layer_norm): LayerNorm()
          (single_cond_scale): Linear()
          (single_cond_bias): Linear()
        )
        (ffw_transition1): Linear(in_features=384, out_features=1536, bias=True)
        (adaptive_zero_init): AdaptiveZeroInit(
          (transition2): Linear()
          (adaptive_zero_cond): Linear()
        )
      )
    )
    (diffusion_atom_cross_att_encoder): AtomCrossAttEncoder(
      (embed_ref_pos): Linear(in_features=3, out_features=128, bias=True)
      (embed_ref_mask): Linear(in_features=1, out_features=128, bias=True)
      (embed_ref_element): Linear(in_features=128, out_features=128, bias=True)
      (embed_ref_charge): Linear(in_features=1, out_features=128, bias=True)
      (embed_ref_atom_name): Linear(in_features=256, out_features=128, bias=True)
      (single_to_pair_cond_row): Linear(in_features=128, out_features=16, bias=True)
      (single_to_pair_cond_col): Linear(in_features=128, out_features=16, bias=True)
      (embed_pair_offsets): Linear(in_features=3, out_features=16, bias=True)
      (embed_pair_distances): Linear(in_features=1, out_features=16, bias=True)
      (embed_pair_offsets_valid): Linear(in_features=1, out_features=16, bias=True)
      (pair_mlp): Sequential(
        (0): ReLU()
        (1): Linear(in_features=16, out_features=16, bias=True)
        (2): ReLU()
        (3): Linear(in_features=16, out_features=16, bias=True)
        (4): ReLU()
        (5): Linear(in_features=16, out_features=16, bias=True)
      )
      (atom_transformer_encoder): CrossAttTransformer(
        (pair_input_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=False)
        (pair_logits_projection): Linear(in_features=16, out_features=12, bias=True)
        (blocks): ModuleList(
          (0-2): 3 x ModuleDict(
            (cross_attention): CrossAttention(
              (q_projection): Linear(in_features=128, out_features=128, bias=True)
              (k_projection): Linear(in_features=128, out_features=128, bias=False)
              (v_projection): Linear(in_features=128, out_features=128, bias=True)
              (gating_query): Linear(in_features=128, out_features=128, bias=True)
              (adaptive_zero_init): AdaptiveZeroInit(
                (transition2): Linear()
                (adaptive_zero_cond): Linear()
              )
              (adaptive_layernorm_q): AdaptiveLayerNorm(
                (layer_norm): LayerNorm()
                (single_cond_layer_norm): LayerNorm()
                (single_cond_scale): Linear()
                (single_cond_bias): Linear()
              )
              (adaptive_layernorm_k): AdaptiveLayerNorm(
                (layer_norm): LayerNorm()
                (single_cond_layer_norm): LayerNorm()
                (single_cond_scale): Linear()
                (single_cond_bias): Linear()
              )
            )
            (transition): TransitionBlock(
              (adaptive_layernorm): AdaptiveLayerNorm(
                (layer_norm): LayerNorm()
                (single_cond_layer_norm): LayerNorm()
                (single_cond_scale): Linear()
                (single_cond_bias): Linear()
              )
              (ffw_transition1): Linear(in_features=128, out_features=512, bias=True)
              (adaptive_zero_init): AdaptiveZeroInit(
                (transition2): Linear()
                (adaptive_zero_cond): Linear()
              )
            )
          )
        )
      )
      (project_atom_features_for_aggr): Linear(in_features=128, out_features=768, bias=True)
      (embed_trunk_single_cond): Linear(in_features=128, out_features=128, bias=True)
      (lnorm_trunk_single_cond): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (atom_positions_to_features): Linear(in_features=3, out_features=128, bias=True)
      (lnorm_trunk_pair_cond): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (embed_trunk_pair_cond): Linear(in_features=128, out_features=16, bias=True)
    )
    (single_cond_embedding_norm): LayerNorm()
    (single_cond_embedding_projection): Linear(in_features=768, out_features=768, bias=True)
    (transformer): Transformer(
      (pair_logits_projection): Linear(in_features=64, out_features=64, bias=True)
      (pair_input_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=False)
      (super_blocks): ModuleList(
        (0-5): 6 x ModuleList(
          (0-3): 4 x ModuleDict(
            (self_attention): SelfAttention(
              (q_projection): Linear(in_features=128, out_features=128, bias=True)
              (k_projection): Linear(in_features=128, out_features=128, bias=False)
              (v_projection): Linear(in_features=128, out_features=128, bias=False)
              (gating_query): Linear(in_features=128, out_features=128, bias=True)
              (adaptive_layernorm): AdaptiveLayerNorm(
                (layer_norm): LayerNorm()
                (single_cond_layer_norm): LayerNorm()
                (single_cond_scale): Linear()
                (single_cond_bias): Linear()
              )
              (adaptive_zero_init): AdaptiveZeroInit(
                (transition2): Linear()
                (adaptive_zero_cond): Linear()
              )
            )
            (transition_block): TransitionBlock(
              (adaptive_layernorm): AdaptiveLayerNorm(
                (layer_norm): LayerNorm()
                (single_cond_layer_norm): LayerNorm()
                (single_cond_scale): Linear()
                (single_cond_bias): Linear()
              )
              (ffw_transition1): Linear(in_features=128, out_features=512, bias=True)
              (adaptive_zero_init): AdaptiveZeroInit(
                (transition2): Linear()
                (adaptive_zero_cond): Linear()
              )
            )
          )
        )
      )
    )
    (output_norm): LayerNorm()
    (diffusion_atom_cross_att_decoder): AtomCrossAttDecoder(
      (project_token_features_for_broadcast): Linear(in_features=768, out_features=128, bias=True)
      (atom_transformer_decoder): CrossAttTransformer(
        (pair_input_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=False)
        (pair_logits_projection): Linear(in_features=16, out_features=12, bias=True)
        (blocks): ModuleList(
          (0-2): 3 x ModuleDict(
            (cross_attention): CrossAttention(
              (q_projection): Linear(in_features=128, out_features=128, bias=True)
              (k_projection): Linear(in_features=128, out_features=128, bias=False)
              (v_projection): Linear(in_features=128, out_features=128, bias=True)
              (gating_query): Linear(in_features=128, out_features=128, bias=True)
              (adaptive_zero_init): AdaptiveZeroInit(
                (transition2): Linear()
                (adaptive_zero_cond): Linear()
              )
              (adaptive_layernorm_q): AdaptiveLayerNorm(
                (layer_norm): LayerNorm()
                (single_cond_layer_norm): LayerNorm()
                (single_cond_scale): Linear()
                (single_cond_bias): Linear()
              )
              (adaptive_layernorm_k): AdaptiveLayerNorm(
                (layer_norm): LayerNorm()
                (single_cond_layer_norm): LayerNorm()
                (single_cond_scale): Linear()
                (single_cond_bias): Linear()
              )
            )
            (transition): TransitionBlock(
              (adaptive_layernorm): AdaptiveLayerNorm(
                (layer_norm): LayerNorm()
                (single_cond_layer_norm): LayerNorm()
                (single_cond_scale): Linear()
                (single_cond_bias): Linear()
              )
              (ffw_transition1): Linear(in_features=128, out_features=512, bias=True)
              (adaptive_zero_init): AdaptiveZeroInit(
                (transition2): Linear()
                (adaptive_zero_cond): Linear()
              )
            )
          )
        )
      )
      (atom_features_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (atom_features_to_position_update): Linear(in_features=128, out_features=3, bias=True)
    )
  )
  (atom_cross_att_encoder): AtomCrossAttEncoder(
    (embed_ref_pos): Linear(in_features=3, out_features=128, bias=True)
    (embed_ref_mask): Linear(in_features=1, out_features=128, bias=True)
    (embed_ref_element): Linear(in_features=128, out_features=128, bias=True)
    (embed_ref_charge): Linear(in_features=1, out_features=128, bias=True)
    (embed_ref_atom_name): Linear(in_features=256, out_features=128, bias=True)
    (single_to_pair_cond_row): Linear(in_features=128, out_features=16, bias=True)
    (single_to_pair_cond_col): Linear(in_features=128, out_features=16, bias=True)
    (embed_pair_offsets): Linear(in_features=3, out_features=16, bias=True)
    (embed_pair_distances): Linear(in_features=1, out_features=16, bias=True)
    (embed_pair_offsets_valid): Linear(in_features=1, out_features=16, bias=True)
    (pair_mlp): Sequential(
      (0): ReLU()
      (1): Linear(in_features=16, out_features=16, bias=True)
      (2): ReLU()
      (3): Linear(in_features=16, out_features=16, bias=True)
      (4): ReLU()
      (5): Linear(in_features=16, out_features=16, bias=True)
    )
    (atom_transformer_encoder): CrossAttTransformer(
      (pair_input_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=False)
      (pair_logits_projection): Linear(in_features=16, out_features=12, bias=True)
      (blocks): ModuleList(
        (0-2): 3 x ModuleDict(
          (cross_attention): CrossAttention(
            (q_projection): Linear(in_features=128, out_features=128, bias=True)
            (k_projection): Linear(in_features=128, out_features=128, bias=False)
            (v_projection): Linear(in_features=128, out_features=128, bias=True)
            (gating_query): Linear(in_features=128, out_features=128, bias=True)
            (adaptive_zero_init): AdaptiveZeroInit(
              (transition2): Linear()
              (adaptive_zero_cond): Linear()
            )
            (adaptive_layernorm_q): AdaptiveLayerNorm(
              (layer_norm): LayerNorm()
              (single_cond_layer_norm): LayerNorm()
              (single_cond_scale): Linear()
              (single_cond_bias): Linear()
            )
            (adaptive_layernorm_k): AdaptiveLayerNorm(
              (layer_norm): LayerNorm()
              (single_cond_layer_norm): LayerNorm()
              (single_cond_scale): Linear()
              (single_cond_bias): Linear()
            )
          )
          (transition): TransitionBlock(
            (adaptive_layernorm): AdaptiveLayerNorm(
              (layer_norm): LayerNorm()
              (single_cond_layer_norm): LayerNorm()
              (single_cond_scale): Linear()
              (single_cond_bias): Linear()
            )
            (ffw_transition1): Linear(in_features=128, out_features=512, bias=True)
            (adaptive_zero_init): AdaptiveZeroInit(
              (transition2): Linear()
              (adaptive_zero_cond): Linear()
            )
          )
        )
      )
    )
    (project_atom_features_for_aggr): Linear(in_features=128, out_features=384, bias=True)
    (embed_trunk_single_cond): Linear(in_features=128, out_features=128, bias=True)
    (lnorm_trunk_single_cond): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (atom_positions_to_features): Linear(in_features=3, out_features=128, bias=True)
    (lnorm_trunk_pair_cond): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (embed_trunk_pair_cond): Linear(in_features=128, out_features=16, bias=True)
  )
)
diffusion_module.pair_cond_initial_norm.weight: torch.Size([267])
diffusion_module.pair_cond_initial_projection.weight: torch.Size([128, 267])
diffusion_module.pair_cond_initial_projection.bias: torch.Size([128])
diffusion_module.pair_transition_blocks.0.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.pair_transition_blocks.0.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.pair_transition_blocks.0.ffw_transition1.bias: torch.Size([512])
diffusion_module.pair_transition_blocks.1.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.pair_transition_blocks.1.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.pair_transition_blocks.1.ffw_transition1.bias: torch.Size([512])
diffusion_module.single_cond_initial_norm.weight: torch.Size([831])
diffusion_module.single_cond_initial_projection.weight: torch.Size([384, 831])
diffusion_module.single_cond_initial_projection.bias: torch.Size([384])
diffusion_module.noise_embedding_initial_norm.weight: torch.Size([256])
diffusion_module.noise_embedding_initial_projection.weight: torch.Size([384, 256])
diffusion_module.noise_embedding_initial_projection.bias: torch.Size([384])
diffusion_module.single_transition_blocks.0.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([384])
diffusion_module.single_transition_blocks.0.ffw_transition1.weight: torch.Size([1536, 384])
diffusion_module.single_transition_blocks.0.ffw_transition1.bias: torch.Size([1536])
diffusion_module.single_transition_blocks.1.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([384])
diffusion_module.single_transition_blocks.1.ffw_transition1.weight: torch.Size([1536, 384])
diffusion_module.single_transition_blocks.1.ffw_transition1.bias: torch.Size([1536])
diffusion_module.diffusion_atom_cross_att_encoder.embed_ref_pos.weight: torch.Size([128, 3])
diffusion_module.diffusion_atom_cross_att_encoder.embed_ref_pos.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.embed_ref_mask.weight: torch.Size([128, 1])
diffusion_module.diffusion_atom_cross_att_encoder.embed_ref_mask.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.embed_ref_element.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.embed_ref_element.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.embed_ref_charge.weight: torch.Size([128, 1])
diffusion_module.diffusion_atom_cross_att_encoder.embed_ref_charge.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.embed_ref_atom_name.weight: torch.Size([128, 256])
diffusion_module.diffusion_atom_cross_att_encoder.embed_ref_atom_name.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.single_to_pair_cond_row.weight: torch.Size([16, 128])
diffusion_module.diffusion_atom_cross_att_encoder.single_to_pair_cond_row.bias: torch.Size([16])
diffusion_module.diffusion_atom_cross_att_encoder.single_to_pair_cond_col.weight: torch.Size([16, 128])
diffusion_module.diffusion_atom_cross_att_encoder.single_to_pair_cond_col.bias: torch.Size([16])
diffusion_module.diffusion_atom_cross_att_encoder.embed_pair_offsets.weight: torch.Size([16, 3])
diffusion_module.diffusion_atom_cross_att_encoder.embed_pair_offsets.bias: torch.Size([16])
diffusion_module.diffusion_atom_cross_att_encoder.embed_pair_distances.weight: torch.Size([16, 1])
diffusion_module.diffusion_atom_cross_att_encoder.embed_pair_distances.bias: torch.Size([16])
diffusion_module.diffusion_atom_cross_att_encoder.embed_pair_offsets_valid.weight: torch.Size([16, 1])
diffusion_module.diffusion_atom_cross_att_encoder.embed_pair_offsets_valid.bias: torch.Size([16])
diffusion_module.diffusion_atom_cross_att_encoder.pair_mlp.1.weight: torch.Size([16, 16])
diffusion_module.diffusion_atom_cross_att_encoder.pair_mlp.1.bias: torch.Size([16])
diffusion_module.diffusion_atom_cross_att_encoder.pair_mlp.3.weight: torch.Size([16, 16])
diffusion_module.diffusion_atom_cross_att_encoder.pair_mlp.3.bias: torch.Size([16])
diffusion_module.diffusion_atom_cross_att_encoder.pair_mlp.5.weight: torch.Size([16, 16])
diffusion_module.diffusion_atom_cross_att_encoder.pair_mlp.5.bias: torch.Size([16])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.pair_logits_projection.weight: torch.Size([12, 16])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.pair_logits_projection.bias: torch.Size([12])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.q_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.v_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.gating_query.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.adaptive_layernorm_q.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.adaptive_layernorm_k.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.transition.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.transition.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.0.transition.ffw_transition1.bias: torch.Size([512])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.q_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.v_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.gating_query.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.adaptive_layernorm_q.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.adaptive_layernorm_k.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.transition.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.transition.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.1.transition.ffw_transition1.bias: torch.Size([512])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.q_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.v_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.gating_query.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.adaptive_layernorm_q.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.adaptive_layernorm_k.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.transition.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.transition.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_transformer_encoder.blocks.2.transition.ffw_transition1.bias: torch.Size([512])
diffusion_module.diffusion_atom_cross_att_encoder.project_atom_features_for_aggr.weight: torch.Size([768, 128])
diffusion_module.diffusion_atom_cross_att_encoder.project_atom_features_for_aggr.bias: torch.Size([768])
diffusion_module.diffusion_atom_cross_att_encoder.embed_trunk_single_cond.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_encoder.embed_trunk_single_cond.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.lnorm_trunk_single_cond.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.lnorm_trunk_single_cond.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.atom_positions_to_features.weight: torch.Size([128, 3])
diffusion_module.diffusion_atom_cross_att_encoder.atom_positions_to_features.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.lnorm_trunk_pair_cond.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.lnorm_trunk_pair_cond.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_encoder.embed_trunk_pair_cond.weight: torch.Size([16, 128])
diffusion_module.diffusion_atom_cross_att_encoder.embed_trunk_pair_cond.bias: torch.Size([16])
diffusion_module.single_cond_embedding_norm.weight: torch.Size([256])
diffusion_module.single_cond_embedding_projection.weight: torch.Size([768, 768])
diffusion_module.single_cond_embedding_projection.bias: torch.Size([768])
diffusion_module.transformer.pair_logits_projection.weight: torch.Size([64, 64])
diffusion_module.transformer.pair_logits_projection.bias: torch.Size([64])
diffusion_module.transformer.super_blocks.0.0.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.0.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.0.0.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.0.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.0.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.0.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.0.0.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.0.0.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.0.0.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.0.0.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.0.1.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.1.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.0.1.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.1.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.1.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.1.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.0.1.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.0.1.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.0.1.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.0.1.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.0.2.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.2.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.0.2.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.2.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.2.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.2.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.0.2.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.0.2.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.0.2.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.0.2.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.0.3.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.3.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.0.3.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.3.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.3.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.0.3.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.0.3.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.0.3.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.0.3.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.0.3.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.1.0.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.0.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.1.0.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.0.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.0.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.0.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.1.0.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.1.0.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.1.0.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.1.0.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.1.1.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.1.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.1.1.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.1.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.1.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.1.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.1.1.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.1.1.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.1.1.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.1.1.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.1.2.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.2.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.1.2.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.2.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.2.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.2.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.1.2.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.1.2.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.1.2.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.1.2.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.1.3.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.3.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.1.3.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.3.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.3.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.1.3.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.1.3.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.1.3.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.1.3.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.1.3.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.2.0.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.0.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.2.0.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.0.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.0.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.0.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.2.0.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.2.0.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.2.0.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.2.0.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.2.1.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.1.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.2.1.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.1.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.1.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.1.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.2.1.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.2.1.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.2.1.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.2.1.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.2.2.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.2.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.2.2.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.2.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.2.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.2.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.2.2.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.2.2.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.2.2.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.2.2.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.2.3.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.3.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.2.3.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.3.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.3.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.2.3.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.2.3.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.2.3.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.2.3.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.2.3.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.3.0.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.0.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.3.0.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.0.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.0.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.0.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.3.0.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.3.0.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.3.0.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.3.0.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.3.1.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.1.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.3.1.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.1.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.1.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.1.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.3.1.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.3.1.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.3.1.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.3.1.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.3.2.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.2.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.3.2.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.2.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.2.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.2.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.3.2.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.3.2.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.3.2.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.3.2.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.3.3.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.3.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.3.3.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.3.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.3.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.3.3.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.3.3.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.3.3.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.3.3.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.3.3.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.4.0.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.0.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.4.0.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.0.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.0.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.0.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.4.0.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.4.0.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.4.0.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.4.0.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.4.1.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.1.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.4.1.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.1.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.1.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.1.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.4.1.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.4.1.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.4.1.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.4.1.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.4.2.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.2.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.4.2.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.2.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.2.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.2.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.4.2.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.4.2.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.4.2.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.4.2.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.4.3.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.3.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.4.3.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.3.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.3.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.4.3.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.4.3.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.4.3.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.4.3.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.4.3.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.5.0.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.0.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.5.0.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.0.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.0.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.0.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.5.0.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.5.0.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.5.0.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.5.0.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.5.1.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.1.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.5.1.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.1.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.1.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.1.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.5.1.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.5.1.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.5.1.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.5.1.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.5.2.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.2.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.5.2.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.2.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.2.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.2.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.5.2.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.5.2.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.5.2.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.5.2.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.transformer.super_blocks.5.3.self_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.3.self_attention.q_projection.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.5.3.self_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.3.self_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.3.self_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.transformer.super_blocks.5.3.self_attention.gating_query.bias: torch.Size([128])
diffusion_module.transformer.super_blocks.5.3.self_attention.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.5.3.transition_block.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.transformer.super_blocks.5.3.transition_block.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.transformer.super_blocks.5.3.transition_block.ffw_transition1.bias: torch.Size([512])
diffusion_module.output_norm.weight: torch.Size([256])
diffusion_module.diffusion_atom_cross_att_decoder.project_token_features_for_broadcast.weight: torch.Size([128, 768])
diffusion_module.diffusion_atom_cross_att_decoder.project_token_features_for_broadcast.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.pair_logits_projection.weight: torch.Size([12, 16])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.pair_logits_projection.bias: torch.Size([12])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.cross_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.cross_attention.q_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.cross_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.cross_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.cross_attention.v_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.cross_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.cross_attention.gating_query.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.cross_attention.adaptive_layernorm_q.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.cross_attention.adaptive_layernorm_k.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.transition.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.transition.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.0.transition.ffw_transition1.bias: torch.Size([512])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.cross_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.cross_attention.q_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.cross_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.cross_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.cross_attention.v_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.cross_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.cross_attention.gating_query.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.cross_attention.adaptive_layernorm_q.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.cross_attention.adaptive_layernorm_k.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.transition.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.transition.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.1.transition.ffw_transition1.bias: torch.Size([512])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.cross_attention.q_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.cross_attention.q_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.cross_attention.k_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.cross_attention.v_projection.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.cross_attention.v_projection.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.cross_attention.gating_query.weight: torch.Size([128, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.cross_attention.gating_query.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.cross_attention.adaptive_layernorm_q.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.cross_attention.adaptive_layernorm_k.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.transition.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.transition.ffw_transition1.weight: torch.Size([512, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_transformer_decoder.blocks.2.transition.ffw_transition1.bias: torch.Size([512])
diffusion_module.diffusion_atom_cross_att_decoder.atom_features_layer_norm.weight: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_features_layer_norm.bias: torch.Size([128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_features_to_position_update.weight: torch.Size([3, 128])
diffusion_module.diffusion_atom_cross_att_decoder.atom_features_to_position_update.bias: torch.Size([3])
atom_cross_att_encoder.embed_ref_pos.weight: torch.Size([128, 3])
atom_cross_att_encoder.embed_ref_pos.bias: torch.Size([128])
atom_cross_att_encoder.embed_ref_mask.weight: torch.Size([128, 1])
atom_cross_att_encoder.embed_ref_mask.bias: torch.Size([128])
atom_cross_att_encoder.embed_ref_element.weight: torch.Size([128, 128])
atom_cross_att_encoder.embed_ref_element.bias: torch.Size([128])
atom_cross_att_encoder.embed_ref_charge.weight: torch.Size([128, 1])
atom_cross_att_encoder.embed_ref_charge.bias: torch.Size([128])
atom_cross_att_encoder.embed_ref_atom_name.weight: torch.Size([128, 256])
atom_cross_att_encoder.embed_ref_atom_name.bias: torch.Size([128])
atom_cross_att_encoder.single_to_pair_cond_row.weight: torch.Size([16, 128])
atom_cross_att_encoder.single_to_pair_cond_row.bias: torch.Size([16])
atom_cross_att_encoder.single_to_pair_cond_col.weight: torch.Size([16, 128])
atom_cross_att_encoder.single_to_pair_cond_col.bias: torch.Size([16])
atom_cross_att_encoder.embed_pair_offsets.weight: torch.Size([16, 3])
atom_cross_att_encoder.embed_pair_offsets.bias: torch.Size([16])
atom_cross_att_encoder.embed_pair_distances.weight: torch.Size([16, 1])
atom_cross_att_encoder.embed_pair_distances.bias: torch.Size([16])
atom_cross_att_encoder.embed_pair_offsets_valid.weight: torch.Size([16, 1])
atom_cross_att_encoder.embed_pair_offsets_valid.bias: torch.Size([16])
atom_cross_att_encoder.pair_mlp.1.weight: torch.Size([16, 16])
atom_cross_att_encoder.pair_mlp.1.bias: torch.Size([16])
atom_cross_att_encoder.pair_mlp.3.weight: torch.Size([16, 16])
atom_cross_att_encoder.pair_mlp.3.bias: torch.Size([16])
atom_cross_att_encoder.pair_mlp.5.weight: torch.Size([16, 16])
atom_cross_att_encoder.pair_mlp.5.bias: torch.Size([16])
atom_cross_att_encoder.atom_transformer_encoder.pair_logits_projection.weight: torch.Size([12, 16])
atom_cross_att_encoder.atom_transformer_encoder.pair_logits_projection.bias: torch.Size([12])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.q_projection.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.q_projection.bias: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.k_projection.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.v_projection.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.v_projection.bias: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.gating_query.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.gating_query.bias: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.adaptive_layernorm_q.single_cond_layer_norm.weight: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.cross_attention.adaptive_layernorm_k.single_cond_layer_norm.weight: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.transition.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.transition.ffw_transition1.weight: torch.Size([512, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.0.transition.ffw_transition1.bias: torch.Size([512])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.q_projection.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.q_projection.bias: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.k_projection.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.v_projection.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.v_projection.bias: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.gating_query.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.gating_query.bias: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.adaptive_layernorm_q.single_cond_layer_norm.weight: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.cross_attention.adaptive_layernorm_k.single_cond_layer_norm.weight: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.transition.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.transition.ffw_transition1.weight: torch.Size([512, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.1.transition.ffw_transition1.bias: torch.Size([512])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.q_projection.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.q_projection.bias: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.k_projection.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.v_projection.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.v_projection.bias: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.gating_query.weight: torch.Size([128, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.gating_query.bias: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.adaptive_layernorm_q.single_cond_layer_norm.weight: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.cross_attention.adaptive_layernorm_k.single_cond_layer_norm.weight: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.transition.adaptive_layernorm.single_cond_layer_norm.weight: torch.Size([128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.transition.ffw_transition1.weight: torch.Size([512, 128])
atom_cross_att_encoder.atom_transformer_encoder.blocks.2.transition.ffw_transition1.bias: torch.Size([512])
atom_cross_att_encoder.project_atom_features_for_aggr.weight: torch.Size([384, 128])
atom_cross_att_encoder.project_atom_features_for_aggr.bias: torch.Size([384])
atom_cross_att_encoder.embed_trunk_single_cond.weight: torch.Size([128, 128])
atom_cross_att_encoder.embed_trunk_single_cond.bias: torch.Size([128])
atom_cross_att_encoder.lnorm_trunk_single_cond.weight: torch.Size([128])
atom_cross_att_encoder.lnorm_trunk_single_cond.bias: torch.Size([128])
atom_cross_att_encoder.atom_positions_to_features.weight: torch.Size([128, 3])
atom_cross_att_encoder.atom_positions_to_features.bias: torch.Size([128])
atom_cross_att_encoder.lnorm_trunk_pair_cond.weight: torch.Size([128])
atom_cross_att_encoder.lnorm_trunk_pair_cond.bias: torch.Size([128])
atom_cross_att_encoder.embed_trunk_pair_cond.weight: torch.Size([16, 128])
atom_cross_att_encoder.embed_trunk_pair_cond.bias: torch.Size([16])

